{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先不管環境假設以及如何取得資料，假設就有一筆“手寫字”的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#載入資料\n",
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練資料: 50000 筆，測試資料: 10000 筆\n",
      "至於為什麼要這樣，簡單來說如果把所有資料拿去訓練，那最後沒有辦法評估model的效果到底是如何，所以一開始必須先遮掉一部分的答案        \n",
      "注意就是Machine Learning對於監督式學習的要點就是要先知道答案，這一部分請從頭開始了解咯~\n"
     ]
    }
   ],
   "source": [
    "print \"訓練資料:\",len(training_data),\"筆，測試資料:\",len(test_data),\"筆\"\n",
    "print \"至於為什麼要這樣，簡單來說如果把所有資料拿去訓練，那最後沒有辦法評估model的效果到底是如何，所以一開始必須先遮掉一部分的答案 \\\n",
    "       \\n注意就是Machine Learning對於監督式學習的要點就是要先知道答案，這一部分請從頭開始了解咯~\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推薦學習清單：\n",
    "<br>1. 台大林軒田教授 https://www.youtube.com/playlist?list=PLXVfgk9fNX2I7tB6oIINGBmW50rrmFTqf </br>\n",
    "<br>2. Coursera Andrew Ny https://www.coursera.org/learn/machine-learning/home/welcome</br>\n",
    "<br>我自己是一開始看到什麼學什麼，後來上了學校資科系的課\"Data Minig\"還有另一堂\"數據科學及大數據分析\"，後來在Coursera砍掉重練,之後會看林軒田的</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "#引入一些必要的套件\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在做任何分析之前，我們就先來看看資料的樣子,python是0-indexed的，也就是說第一筆資料是從0開始算，然後用[0]去呼叫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.01171875],\n",
      "       [ 0.0703125 ],\n",
      "       [ 0.0703125 ],\n",
      "       [ 0.0703125 ],\n",
      "       [ 0.4921875 ],\n",
      "       [ 0.53125   ],\n",
      "       [ 0.68359375],\n",
      "       [ 0.1015625 ],\n",
      "       [ 0.6484375 ],\n",
      "       [ 0.99609375],\n",
      "       [ 0.96484375],\n",
      "       [ 0.49609375],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.1171875 ],\n",
      "       [ 0.140625  ],\n",
      "       [ 0.3671875 ],\n",
      "       [ 0.6015625 ],\n",
      "       [ 0.6640625 ],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.87890625],\n",
      "       [ 0.671875  ],\n",
      "       [ 0.98828125],\n",
      "       [ 0.9453125 ],\n",
      "       [ 0.76171875],\n",
      "       [ 0.25      ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.19140625],\n",
      "       [ 0.9296875 ],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98046875],\n",
      "       [ 0.36328125],\n",
      "       [ 0.3203125 ],\n",
      "       [ 0.3203125 ],\n",
      "       [ 0.21875   ],\n",
      "       [ 0.15234375],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.0703125 ],\n",
      "       [ 0.85546875],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.7734375 ],\n",
      "       [ 0.7109375 ],\n",
      "       [ 0.96484375],\n",
      "       [ 0.94140625],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.3125    ],\n",
      "       [ 0.609375  ],\n",
      "       [ 0.41796875],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.80078125],\n",
      "       [ 0.04296875],\n",
      "       [ 0.        ],\n",
      "       [ 0.16796875],\n",
      "       [ 0.6015625 ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.0546875 ],\n",
      "       [ 0.00390625],\n",
      "       [ 0.6015625 ],\n",
      "       [ 0.98828125],\n",
      "       [ 0.3515625 ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.54296875],\n",
      "       [ 0.98828125],\n",
      "       [ 0.7421875 ],\n",
      "       [ 0.0078125 ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.04296875],\n",
      "       [ 0.7421875 ],\n",
      "       [ 0.98828125],\n",
      "       [ 0.2734375 ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.13671875],\n",
      "       [ 0.94140625],\n",
      "       [ 0.87890625],\n",
      "       [ 0.625     ],\n",
      "       [ 0.421875  ],\n",
      "       [ 0.00390625],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.31640625],\n",
      "       [ 0.9375    ],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.46484375],\n",
      "       [ 0.09765625],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.17578125],\n",
      "       [ 0.7265625 ],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.5859375 ],\n",
      "       [ 0.10546875],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.0625    ],\n",
      "       [ 0.36328125],\n",
      "       [ 0.984375  ],\n",
      "       [ 0.98828125],\n",
      "       [ 0.73046875],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.97265625],\n",
      "       [ 0.98828125],\n",
      "       [ 0.97265625],\n",
      "       [ 0.25      ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.1796875 ],\n",
      "       [ 0.5078125 ],\n",
      "       [ 0.71484375],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.80859375],\n",
      "       [ 0.0078125 ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.15234375],\n",
      "       [ 0.578125  ],\n",
      "       [ 0.89453125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.9765625 ],\n",
      "       [ 0.7109375 ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.09375   ],\n",
      "       [ 0.4453125 ],\n",
      "       [ 0.86328125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.78515625],\n",
      "       [ 0.3046875 ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.08984375],\n",
      "       [ 0.2578125 ],\n",
      "       [ 0.83203125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.7734375 ],\n",
      "       [ 0.31640625],\n",
      "       [ 0.0078125 ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.0703125 ],\n",
      "       [ 0.66796875],\n",
      "       [ 0.85546875],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.76171875],\n",
      "       [ 0.3125    ],\n",
      "       [ 0.03515625],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.21484375],\n",
      "       [ 0.671875  ],\n",
      "       [ 0.8828125 ],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.953125  ],\n",
      "       [ 0.51953125],\n",
      "       [ 0.04296875],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.53125   ],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.98828125],\n",
      "       [ 0.828125  ],\n",
      "       [ 0.52734375],\n",
      "       [ 0.515625  ],\n",
      "       [ 0.0625    ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ]], dtype=float32), array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]]))\n"
     ]
    }
   ],
   "source": [
    "first = training_data[0]\n",
    "print first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面那種看法太辛苦，只看到是一整串的值，所以就要想辦法了解一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2L,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape(first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從上面的結果可以知道，first是一個2維的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784 10\n"
     ]
    }
   ],
   "source": [
    "print len(first[0]),len(first[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然後first的第一個資料的長度是784,第二個資料的長度是10，實際上呢，第一個資料表示的28 * 28的pixel,第二個是記錄這個手寫字是什麼號碼\n",
    "<br>在把第一個資料還原成圖片之前，我們先看看這是什麼數字好了!</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二個資料（也就是first[1],再次提醒是0-indexed）顯示第6個儲存格的值是1,所以很有可能的，這張圖片就是5(0 indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xd2066a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztfVuIrNl13re76159O3M0mgkaW07ISyCIIcF6UcBtHIwI\nBgU/KIogSI4RfogSE+tBil7mKPGDrYcBReAHK7LQBAvfwJEciCOHpDF6cCQ7USLHI8uQjGzZmjO3\n06e7rqeqeueha/39/avW/qu6qi/V9a8PNv+u6qr6d3X3t9fa6xpijHA4HOXC1m0vwOFw3Dyc+A5H\nCeHEdzhKCCe+w1FCOPEdjhLCie9wlBArET+E8N4QwrdDCN8JIXz8qhblcDiuF2FZP34IYQvAdwD8\nGIC/AvANAB+IMX5bvc4DBRyOW0KMMVjPryLx3w3gz2KM340xjgD8GoD3JW6ejRdeeCH3eN2Gr29z\n17fOa7uO9RVhFeK/A8Bf0OPvTZ9zOBxrDjfuORwlRGWF9/4lgB+kx89Nn5vBgwcPsvnBwcEKt7x+\nHB4e3vYSCuHrWx7rvDZg9fUdHR3h6OhoodeuYtzbBvCnODfufR/A1wH84xjjy+p1cdl7OByO5RFC\nQEwY95aW+DHGSQjhowC+ivMjw+c16R0Ox3piaYm/8A1c4jsct4Iiie/GPYejhHDiOxwlhBPf4Sgh\nnPgORwnhxHc4SggnvsNRQjjxHY4SwonvcJQQTnyHo4Rw4jscJYQT3+EoIZz4DkcJ4cR3OEoIJ77D\nUUI48R2OEsKJ73CUEE58h6OEcOI7HCWEE9/hKCGc+A5HCeHEdzhKCCe+w1FCOPEdjhLCie9wlBBO\nfIejhHDiOxwlhBPf4SghVmmT7VhDrEOfwhhjtg6Z8+NF3p+ahxAKh76nNVLrWmRt8+7P60g9Xgc4\n8UuOq94oYow4OzvDZDLB2dlZbkwmk5n7WY+LRggBlUoF29vb2N7enpnHGDGZTAoHr09f9Xo0Ube2\ntmbuq9eytbWVvY6vW1tbTnzH7WEe+VbB2dkZxuMxxuMxJpPJzPzs7Cx3Ty3RZchmoedbW1uo1Wqo\nVqszVyHVeDzGkydPMBqNzCFr4rkMWR8TlOfb29vJ+9dqNVQqlZlRrVZRqVSwtbU+J2snfslQpEZf\nBUR6psjHUt+6WloCzyuVCur1OhqNRjbk/dvb2wCA0WiE4XCYG4PBAMPhEE+ePDHHaDTCkydPMJlM\ncqq5vlYqFTQajWwNvJZ6vY56vZ7bCGRtIYRsfeuAlYgfQngFwGMAZwBGMcZ3X8WiHNcDi+ip67IQ\niS9EErLJdTweF561heCpUalU0Gq10Gq1chJaNIEYY3bvfr+Pfr+PXq+Xzfv9fm4j0NfxeJw8l4cQ\nUK1Ws/vzaDab2VU2ACa9HEPWBatK/DMAhzHGR1exGMf1IUX66yD+ZDLJEb/f72MwGGAwGOSIbw3Z\nOERr0MeFarWa20BEktZqNUwmEwAXEr/f76Pb7aLT6WRX3gT0htDv9zEejwGkDXP1eh07OzvmSB0Z\ntre3TfvGbWJV4ge4S3DtYf3DpSzaV0l8kaRMtNFolCO5Jj4T3TqHV6vVjFhCqmq1ikajkST+6ekp\nTk9PcXJykm0CqcES37LONxoN7O3tYX9/H3t7e5kGIWtiA+HW1lZ2xpeNYF2wKvEjgN8LIUwA/HKM\n8XNXsCbHFSJlNdePF3VnzYNW9YX4QqwnT57MEJ4fy6aRGqxCC+nr9XpmPwBs4h8fH+Px48c4OTnJ\nNoJOp5O7np6eYjQaFbrpWq0WDg4O0O12Z0jPEPVezvqWx+A2sSrx3xNj/H4I4WmcbwAvxxi/pl/0\n4MGDbH54eIjDw8MVb7u+WOaPm3rPZc7kKSu59X62llvXVfDkyZMZKcpS9smTJ6a0l8dMfNlAeF6v\n1zPXmZCqVqvlztW9Xg+dTicj9MnJCU5OTnB8fLwQ8cXtZl3Pzs6yzabZbGa2C1mjdmNe5aY6D0dH\nRzg6OlroteGqFhRCeAHAaYzxRfV8XKed7rpxWat5Sg231OCUq0s/x59rGdHkH9Qaq/6tRqMRer1e\nNkTFl/loNCr8brKO1Dm/Xq/j3r17ODg4yAY/jjHi+PjYHCLxe71ebmPix+Px2PTFy7zVauGpp57C\nvXv38NRTT+Xm9+7dw/7+Ptrtdnbul7lca7XaSr/fyyCEgBijGTiwtMQPIbQAbMUYOyGENoAfB/Cp\nZT9v07CI4ayI9JrQlotLB6TInO+v1yEk0pJUrqueRcfjcabei0FPhhjPiox7skbLlSe/k16vl/nG\nhZj8nYXkMkTiyxlfbA5iJJQNT2wG1pB7NZtNNBqN7MqDLfrs01+34B1gNVX/GQC/HUKI08/51Rjj\nV69mWXcbluHMUvcs4lvk5quWgDpARv6Ji0iV8mWLH3sVjMfjnAtPz8Vqbm1KlmZjaTpMeg7TlZ+f\nnJzkiM9n+263m/Pvc2yBqPNCdGtoolubQL1eR7VaRbVazTaOdSI9sALxY4z/D8DzV7iWjUOKfPxz\nDSa3Vnu1tLaGRXwmEFvbU37sVaCNczqIJxXAk9KKrN+PJr08L5ujSHct7U9OTrLjBq+NiS9SnyPu\nhMTiPUhJe5mL3WFTJb6jAEWSi1+j38ORb9aQf9jUYFXasgOMRqMZ37Xlx14WHLJraSd8lLA2gHnJ\nL2dnZzPqPd+Tic+GPRmDwcCM3U9JfCGxGBK1lLc2AY7e08eRdYET/xqQUl+11dyaW5Kd56w2W0P8\n5KlY9+FwmBmzxOjGBq7RaLTyd7eSdHQSTOrYw+QTScnGNfHfTw1XubgBkd5ioWf/vcyHw2FyQ9bE\nZ6+BeA4s0vNVzvgbq+o7imGRfhHis/TSKqmQXhvN2Jimia/HcDjMXF3WuAriWxuf/lkKnH1nZeEJ\n8eV3xZuk2Ci0i0676+Q+VoCOBN2wis+kX2TohB1X9TcAKUu8gC3s+myure7WVYxvmvAW8bXlXFT1\nFOlvgvirYmtrKyOO9qELkUQb0IY9rbrL54kUr1ar2XPymXrOSTiciCNjd3c3c9VxnL6l4ltrXRc4\n8ZdESpJJVhpngvFcG9/0+7VxTBvzLFVfDHMcRWap+6zqS9SZ3GNdIsuEfLVazcyCYxJaQ2wAsnmI\nCt5ut9Hr9TAej2fIzkPuLSTW13a7jf39/Sxkt91uZ8RPkV7ut07kd+IvgSLX02g0wmAwSCaDcKx6\n6nNSRr1VjXsxxiyMltNURRtZB4iqL6TVWXBsNRcy8ogxmqQX7WgymZiEZ62CP0+f85vNJnZ3d7Mh\nkl9U/LtAesCJvxQWsZpLqKqcMWUuIaspqz+fW60jg1VUwnKXpaLjdK76ukp8CYnl7Lfd3d2cZOUz\nuMxjjBnpLePnZDIxDYcs8fmz9Wg0Gmi327khRj1Zh1V5Z93gxL8ktHquz9Ai8bvdbhYfLkEkx8fH\nGAwGScJri7hVKioVwMPurNRRIsZ8AI9oD+tCfJGMInVFWu/t7WUZcc1mMydZ9TzGmCXtWHYSdgem\nwnL5M/V9ZEPiwZqIlvYu8TcImvBMTib+48eP8ejRI7z11lt488038dZbb6Hf7ydj7VPWeOt+Vg25\nefYD6yjBySW3SXwhhpb47XYbu7u7ODg4wFNPPYVWqzVj6edrjDEZ1SjHoKJY/NRns4svZfwT4qeS\nfNaJ/E78JWC56YR4coYWif/o0SO88cYbeO211/D666+j2+0WkrroGGC5Bq1NRNbIV163FQvPgTU3\nDSYEn7NZ4ksiTLvdNgtZcpBMKuxZviOTPPVZqbmszaq5J1qBfCer8u66wIm/BFKk1xJfiP/666/j\n4cOHePXVV3F6elrobptHXH0ter1es17/ZX3s1wFNCG3cE+IfHBzg/v37GfG1RNUus9R35CAdTW5L\nRbeCiCxNQ0fopYp1rguc+CsiJVGtQhQSRGJlnVkBPreJooKTRa9fZHOyPker+pzz3mq1MpV/Z2dn\nbljvvO9VJOH1udzaZOaV0L4LcOIvAR3pJdLfkg568D9GjDF7/zpBS7yUVLU2hXl2Cvmuqc9h6zqH\nzXLkXCrqblHiW+d7eawJbz1ed8PdInDirwD9jwEgqTby80BeFV2nfx5NDC0VLZLpJBptfLSMhylN\ngu+lQ2aF/Pqe87SR1PdL+fOLxrzX3hU48ZeA9c8g/9QpsvNjbahbp38Y+U767Mpx5ylJKMTXBT74\nGCAGtnnEL5L4vE7rM+Z9vyJtpuj4kFL/1+nvtyic+EtCk14k/iKGIyE/Z5qt0z8Pk4+t1RycklKB\nOdeAjwaiCWgSyVyu+t46M46Jz1c9T6Fo0yoiu37vXZb2gBP/0kipf4IilxBLfNYM1knqF5GvVqvl\niG9Zv6XpxXA4zEl6CZXlM36KXEXkr9frS5OeXzeP7POud5XwAif+krCIz/+0ltVXhhgC9T9OCGEt\nDH38PfQZm9NMrSFJMEA+a47LVss9Umr0PFXfIttlCLgowa3PXvQ96w4n/hIQgso/qhCZo8Isaz5f\nNfnX5R8nJXGFeNVqtdAqLmd6dm1y5VpR91OSdlHjnrz+Kr7vZZ6/ynvfJpz4S4L/8EJ6UeF12SbO\nMuMMOzaeyZmYI+hS0sSK6LtsDEBKinFaKveDk2utVkuSXiS+ldwi33VeaS/OuNNRcTIcq8OJf0UQ\n8gjxObtsb28Pg8EAT548Qa1Wm1vltsh4lkrb5Vj0eSj6/Hq9ngXMpOrCWyo+E7+oph/X1bf8/XIv\nyXHnFtN3XcquE5z4S8KSwiFc9HITAu3s7GQlsSaTSUZ8q/T0cDg0s8eYWGI11+8HLizn89ZdZIsQ\n7YRzznd3d7G3t4fd3V3U63XT983GPas0mAzO/7eSaCS/nVNdRStyXB2c+EuAjXB6A9ja2sqVYd7Z\n2cnlydfrdbOstczPzi7KR1tjMpnkOtT0+30AF5V7Fl2//lxuGCHSXVJheTQajblW/VR1IK5ElCot\nJtqGEN8l/vXAib8C2Mgn0BKfSS8SNdVpRkpHpZo5yBlZCnuI9Vy0gEX92CzldU67SHwhvrSokjZV\nzWbTDGCR+dnZ2Uy5MV2CTG92fNV95nWqq+Nq4MRfEhbpgdkzPjdrEGIJ6fnsK4/Pzs6S1V8qlQpG\no1FmWQeQK/5xGV+2rNOqMMPE39/fx71793D//n3cv38fzWZzhvSa+LqRBs/199alyer1+kwdO6uB\nhmM1OPFXAAeosHFPJL4mvWwGuh5fo9FAr9fL2ikX1XwbDofZeVek62AwyKTiImtml5kOktGqvhTA\nuH//Pp5++mm0Wq1kEBMT36oVKH31dDfdRqOBbrebbT4s8VnVd1wdnPhXgJSqz6QXUsmQZhZcvUWI\nz8/p+WAwAICZ3P9KpXJpiZ/y1Vuq/tve9jY8/fTTaLfbSR+8aEFWp1u5DgaDXLMLrqEnm1G73c6V\nq3ZV/+rhxL8kFgnsEPJzxRchv67jpocUi+SS0fxYLONyLu71ejkJafXOAy6yAbWar7vDcHFLbvfM\nz1mEl7kE7eiSYDIGg0HScCm2B3bpuap/PXDiXzGs0NNUpRtL8gKYUe95bG1tZeWi2T4gxrJKpZIk\nnuQEcO04Ue3Fmr6/v4/d3d3sOdl4dHKOrJ+/t/4d8O9CwAFCXORTNsytra3s/uzLd+JfLZz41wAm\ntCY6gBnJy2QQiZ8y8Gnia3eZBNHoszXfl48jfKaXopZS6UYb2FjdnhejbpEeQG7T0aSvVqsIIWTa\nBVv2nfhXCyf+FUO7y3Q2mtYIhNCicscYC8tHM/G5MYbUyN/a2sq5ztj6L3H02uWoS1hLhxjdKGIR\n0qckvfxMNjmL9JJ5x2HCYuBz4l8t5hI/hPB5AD8B4GGM8V3T5+4B+HUA7wTwCoD3xxgfX+M67xSY\n2PKYNwQr803cXTFGs5ijXLe2tmYIz0ExIYQZKz/nwgvROB6fpb2o+pbEtSS+nuvnOHNRtB/peKNJ\nL5l3um2W3ngcq2MRif8FAJ8F8BI99wkA/yXG+OkQwscB/Kvpcw4gJ+14E+DgnGq1asbaA7M5/Tq7\nj1V8Jr1IdE167hfHZOMzvkj7g4ODzLimJb5lWS8ivYCTmMTmoUnP67c65LjEv1rMJX6M8WshhHeq\np98H4Eem8y8COIITH4BNet0xRwxwVrccIF++ywqLFcu+Jr1sHJr0o9EoIw5rG1bTiv39fbM1lKXq\nF/0OeK7LWzPpre+vQ5bdqn/1WPaM//YY40MAiDG+GkJ4+xWu6c5Dn2+1RZ/TaPVVQntTATJbW1um\npNcFLYX0kv7LhTKtRKLd3d0sHp/jDdiqfhlfOlv/OWNQpL+4Li3Xo/W9+fMcq+OqjHuFuaAPHjzI\n5oeHhzg8PLyi264f5knFlH/d+sfnuTyWDD9dnEJGs9nM0n9FRdfSUm8q2iDJWsZlSbeI6u+4Hhwd\nHeHo6Gih1y5L/IchhGdijA9DCM8CeK3oxUx8xzlYBU6dm1OEKyKuRVgeXBXHavghGYB8DpejiWO9\noYXqpz71qeRrFw2ADtMh+AqAD0/nHwLw5csssOywLOJasqZew88x4TX5UyqyHCu400+v18t1+ul2\nu5kBcZ266TquDou4874E4BDA/RDCnwN4AcAvAPjNEMI/BfBdAO+/zkVuKkQC85V/VvS+Iomv02b5\n87TEHw6H6Pf76HQ6uUAZDjPm1GLHZmARq/4HEz/6+1e8llKhiPT8Gj0vkvqLSnzd208yAzlRhhtX\nrkMbbcfVwiP3bhGa9PPO/Pz4smd8gaj6+ozPhkAOq3VVfzPhxL9lMOkXVfXl59YZXz83T9UfDAY5\nd51E9UmlIHEZusTfLDjx1wSX9Y8vcsafp+oPh8OZQhfb29uZf9+Ne5sLJ/4N4yp82kJ0VsvZn69z\n+Tm7T8p3AxdluySrDziPmmu1WrmwYMkjkCi7Is+D427AiX/HIL51iWUX4xtHAHLZK13wcjQa5UJw\n5fVSortareZSfnVOgKTO6qg6WZvjbsCJf8fAQTVWMQuR4pr0nMknlnsxLkqMvxT6lJqAOgNQPleO\nE1xXwEtj3S048e8YhGgi8XWUnRCZyc+lrLUNgGvkjUYjVCoVM9dfyM8punKVdaXcko71gxP/joEl\nvs5rr1TO/5ws7XVzCyGoThaSsb29XajqSwss7mzD4ceOuwEn/h0Dk1zntddqNYQQZkgvBO73+5mE\n56AcroIr+f5Fqr6ONxAtxHF34MS/Y2BVn0k/Ho+zCjYW4fv9PlqtFs7OznK99oT4slmEEGa6/GiJ\nz2sRuwJnFzrWH078OwYhu1yly6yo7SLxU+RnQ56o7dKCq9/vI4QwY9zjGn5MfDbyOenvFpz4dxBs\nyBMVW8g3Ho+zQpXcrVeCceQ9HMjD6bp8TODMvW63m/Xrk3oAYuyTwS2+ZZ18lbn188sEMDlWhxP/\nDoKNadqaLll1XFZLou/EeMfFOThuX0jLGkC328Xjx49Rr9dRqVSy5p5cDITnuv6+NZ+XVOS4fjjx\n7zCsDD9Op5VqPOLrF01BSKaTdXRPPknXFdKHcF7BV3f64SHhv1zJR8+tIZuR42bgxL/j0OQXw59I\nfJH0Qnomt5BesvPkZyzxO51OLspPOtpy+WsuAaZTey2C614BAHJS33H9cOLfUaTUfS3xtctPDIJM\nem5aIfH4o9EI/X4/R/rRaJR1900N3R/QIvp4PM5iEGT93HzEcf1w4t9haPID+TM+k17i+re3t3Pq\nvRBZuvQI8eXnTPrBYJCrvstXmcsmkhrcKkzWLZuR4+bgxL/j0Cm3THyW9FJUQ1psSXttqZ0varlk\n4YlPX7QDeb0QXdpc6SFSnwc3AeW8fs4ylHLbjpuBE3+DkAru4fJZ0pBDimsK8blbjRCfJb1sDJKr\nL4Ndh0L+lPFPJxTJJiUNNZz4Nwcn/h3DIpV5hFDcukvGeDzOuuPKlQffQ2f7SXCQhPfK8zo3gGsB\naPKLwVHWw2uWc74uIOJ+/quHE3/DUFShB0B23pdmmXt7e+j1ehgMBlnwj0TzCTm5xZVsBOwW5HJe\nw+FwRtXn0Ww2Z4p8cN5ArVZL+vktX78TfTk48TcUmvRnZ2e583+j0ciaZQ6Hw8ztJ75/luIyF8Of\nGAc16bmcV2q0Wq1c+K+QXlT9RqOR7J0HXPQFtL6vY3E48TcQ2r0npJfIPZb4QnpRuxuNRiaRJb5f\nn/sl3l+TXrvttFtPynppSc8q/2Qyyd4rV91ajI8DXgNgOTjxNxRMBt2mWojfbrezRB3g/BjQbDbR\n6XTQ6XRyAT2S0SettLjMl6j+VtCOftxqtZKkl01KjgVs8LNKfcnzqbLkjjSc+BsKixya+BzKyw00\npMoOcBHFJ4Y3OYuLFmHF3heF68p95XNYM+EKQnpTECOlpeo74S8PJ/4GIqUOi6rPrjUAOdJLog2Q\nD93ljL6iSrvzknOkJgCn8nLFYFH3+Wfs67fI78S/PJz4GwrL8s0Sn5N2hPTtdjsL/GHS6zh+q6+9\naAF8P+vKxAcuIg0lyEc2GHm9HBGE9KxtyGtc1b88nPgbhqJ/fvbxS6QcB/rU63UAyHLx5SpGvl6v\nlxFPJL8MTX6W6Dw/OzvL+fklsk+Mf1wajDUDbajUx4hFE3x8cziHE79kEPWbjW8cJy+Sf3d3dyal\nt1KpoNPpZEE7OohHwn05YEgPIbZ06e12uzmNQsKJrZJf4/E4Cy/mwYVGrQ3AyT4LJ34JwcY0qaUv\naDQaaLVa2N3dzZXuliNCt9vNVe7l0lziphMXH2sEEifANf4Gg0FW1Qe4sClYBT7l88T4KEcDuQIX\npGcXn7v8bDjxSwhWlbW7bDKZoN1uz9TrF09Ap9OZ8fPLqFQqOekspAWQk/raaCg/5z4ALOnl88bj\nMVqtVhaAZFn+rfBeJ/8s5hI/hPB5AD8B4GGM8V3T514A8BEAr01f9skY4+9e2yodVwa2tIt6zS45\n8c1rSS9+f6m/x4OTfKS2n0T2AReBPjLnUl/ABelZxdeEZ82h2WyapC/K8HPS57GIxP8CgM8CeEk9\n/2KM8cWrX5LjusGx+7wJMDl1y2wJ7+10Ojg9Pc2uQnqrRbd8Fhfh5DM+kCe9aBJsM2CfPhsVLdLL\na3R+gmMWc4kfY/xaCOGdxo98C72DsEgvRjfJ6NNVfNrtdnbuluKbjx8/zs7Y3IdPPhe4IH2lUsmy\n+zjkV6f9VioV9Hq9me68THzL/8/txMTIJ/fn7+24wCpn/I+GEP4JgD8E8LEY4+MrWpPjmsHkl6Ae\nmQsBuRMvn9m5eAdb4zmqTx5zSC+r9XKVzYCDfSSyj9V7TXirgxCn+vJ3k+/rZ/w8liX+LwH41zHG\nGEL4eQAvAvjp1IsfPHiQzQ8PD3F4eLjkbR2rwoq4Y3BgjxTJ4CHnaO2i44Ye7GZjX3sIIcsN4PeI\nTUE+L/Ve1iR0lCC7JrmpJ3+OZfjbJBwdHeHo6Gih14ZFqp5MVf3fEePeoj+b/jx6ZZW7A+sszdd+\nv4/Hjx/j5OQkU/n5cafTQa/XSw7JBmTDHc8rlQp2d3exs7OTu/J8b29vZsjzrVbLrPMn8zKV8J5q\nOuZOt6jED6AzfQjh2Rjjq9OHPwngj1dbomPdkDKOcT4/Z/aJXaDRaGQtuKwrRwTqTr5yVOCiHmJ3\nAGZdglYloH6/n1X8lXXK9+Hzf9mxiDvvSwAOAdwPIfw5gBcA/GgI4XkAZwBeAfAz17hGxw2CVWht\nHQ8hZBJUKvVwGLCU1tL+fR4s/bvdbi4HQHz+Mh8OhznSW649Xf5rMBjkagICF5uSa54XWMSq/0Hj\n6S9cw1ocawRtAJTnhOBa0stm0Gq1cq45brktIbqnp6dZ6i8nBInVX4ivSc/Wfm105Jp/8hq9Pif+\nBVz3ccyADWCSLcc5/Rwiy6q/EE+32ebBpAcuwnQHg0EWOSgWfwC5SD8JDioq9CklwnSpcfb9O5z4\nDgOpCjcsMZlUbJzTSTZ63mw2zXx/ycXnKD/u+CPWeS7bxSG+8vmpqEN53nEOJ77DhHafCbTrTA85\nm6cMeGJsYykuGXpc7IMj8Hg9tVotJ+n154/H49yRhBuHOvEv4MR35GD5ufk5zo23inGMx+OcG42z\n9xqNBkIIWesu6cTL0X8AClVyLvklj+U4IKTnbj+y+bBR0CpSUvT9NxFOfMfSsEjCIb8c4COvlzbb\nUnxDB9jw51oSWkcEViqVzPofQkC9Xs/V7bdy+tljIffb9OAeDSe+YymkSl5J1J9E4Om0X/GvM/lT\nUXV8DwFXAOIsP3ltrVbLeRKs3H5OKNLuyrLAie+4NHTsO8+Lcv2l0CdH0onUt2ropcAuP9kwZEMQ\n4hdV8ZE1iuVf1l0mOPEdK8GS0lauvxgEWeIXqfo6wUagJT6f9SeTSY74ukqQkF/uaWUTlkXqO/Ed\nS8Eipw7zZdILMa0im1axzJTkZ+LzY+7mw6q++P45v18MkeJCLGPuvhPfsTRSklJIrLPwYoxZhV2u\nrMslsyzC6+eE+LwJcG5+kao/Go1yGxZ7KMoEJ75jZaQktQ75jTFmxTK1Zd9S9YuMe3I+DyHkLPXV\najVLCrKMe1IEhEmv8/3LACe+I4eUeq0f62g+TRzr5wDMOnrLEk8X5pC4fyu+gEfqe5YJTnyHiSJy\nF9XM11c9f/z4MR49eoSTk5OsYi+36db3syCS2qrzxzYEHpyfz0cMq9BHGeDEd8ygSFqyRV1X59H1\n8azmGkz8breLfr+fa9xhkZ7nrKJzlR0uDmoRPhU0tGgHnk2DE98xA010vor/XFfPsVR43VBjMpng\n9PQUx8fHuWo9IvG5/RavRYNJz510OE5AN93QcQNlJ78T32FCq/TabSbGMraYswEtNTqdjqnqi7V9\nHumtOns8arXaXPLr3ntlU/MBJ75DwVLteVjpsDpQRtfF58cW8VnV53WkwMQX8guprTO+lvjaPuCx\n+g4HYKr5cl4XInOuPVfcYfKLNsCPuS6/nPFZ1Z9nbbckvkj1eUY9GVZ5sTKRHnDiOxKwJD6XvJJc\n+lQxTa6Iw9qApONKNx7Lqj8P+ozP5C9S8+W1OgtQz8sAJ/6GIUUefXbWLjomOhvp+Hw+mUwwHA6T\npbMlVFZlzOIlAAAR2UlEQVSTXRfL6Ha7uSAbbq89D1ria1WfSa5ddzIcTvyNRconbrnauG4+W+ut\nITX1UoNDZC2VnyvzyNn+svXw2J2nN4AiH33ZpHoRnPgbiEUlumWB10TVQ4ibKqjJnW4tA58uyyU/\nX+R8D+SLZuhuOVYHHzbgOS7gxN9gcMScjBS5iyz1WlUvmosUTw3L6Mdhuyno87iW+FqdL1PrrGXg\nxN9AWFKerfNa5dYEtqS5lfTCaj1b83XQjg7ksTSCIlVfk57HPIlvlfRyOPE3FhbpLXccd7jRHW+s\nuRjjUgE83LVWh+ymovmKJL5V6GORMz7/3FX9WTjxNwyp+HomHxvppJUVD25xpa98htfSmw11qUSd\nVIKPRfxUBVwrXl8b9sqcgLMInPgbCk36VABOt9vN+dVlyPP6Kj731JiXaGOlxy5q1JOrJr028vEo\nc5BOEZz4a4Z5JEi56eSaUqVl3u/3M5Kfnp7OjBTpmfhFn79oEE6qtDUTW8+3trZQq9XQarXQbrcL\nr1zwgzvuOvnP4cRfc1iBN6kEGt1c0hqDwSAn2ZngMueAHMvnrtNvL1vUwipvzVJcZ9yx+l6v13Pd\ncPXY3d3FwcEB9vb2sLOzk20ATH6HE39tURSAk8qDl/N7KnJO+tTxmZ3P9XJNxd9rY5w+yy8CtsZb\n+fQcfmtduUsOX2W+s7ODvb097O3tod1uo9lsol6v50J1HU78tYZ1DrYi7PiMnUqeYQs+96nXc12W\nmjcNPsdrjWPRszqAGUs8X2u1WlaCm688F6Lr0Wg00G630W63sbOzg52dHTSbzVxLbsc55hI/hPAc\ngJcAPAPgDMDnYoz/NoRwD8CvA3gngFcAvD/G+Pga11oazIu8E8nObjUrcaYonp571qey6/Tn62Cb\nZWrlaZWe4+yl7r6W6Fq6y2tkQ9BzPUTVd+JfICyQBvksgGdjjN8MIewA+CMA7wPwUwDejDF+OoTw\ncQD3YoyfMN4fL3sGLDOKyl5J5J2lwnP2W8oiL9lwqcAdLdl1kg5b7TXxF90AxEDHKjzPm80mdnZ2\nclKbH7darZwWYGkFkqHHmXrcxKMsmFYpNne7ub+FGOOrAF6dzjshhJcBPIdz8v/I9GVfBHAEYIb4\njuVQ5I/nszz3o5cutCcnJzlLPT/u9XrJqDvOi08l8lhEv4zU1xKfC2hIW+t2u52d03d3d7P53t5e\njvjyHn7MGXl6uHHvApfa/kIIPwTgeQB/AOCZGOND4HxzCCG8/cpXV1JYlnwddpuKwBM3nRS70EPS\nZlNHBe5QswjBL6vNsXFPl8tqNBqZS253dxf7+/s4ODjIjXa7nb2eJTtLdMsV6H78PBYm/lTN/y0A\nPzuV/PovnvwPePDgQTY/PDzE4eHh5VZ5h7AoEVIEijHm0mT1XCzzVkitFLkQkp+cnJjEtwpkLhIz\nzxASaf94qrqNjEqlkjuT8xA1X5PdIn5qlDnf/ujoCEdHRwu9du4ZHwBCCBUA/xHAf4oxfmb63MsA\nDmOMD6d2gP8WY/xbxntLdcafJxFTEpQl+zw/PBvk9Oh2u2ZgDle8sc7uMhY9p6d88EVRdNLiyiK9\nDFbzeYjK32q1ZopucJWdMhNfY6Uz/hS/AuBPhPRTfAXAhwH8IoAPAfjyKovcNKTCVQE7XVaGlbOe\nyp7TG4Cc8bV/ngtksNFumS42RTXtU9VwNEkto5w8Fl88G/V0NJ5VcMPV+MthEav+ewD8PoBv4Vyd\njwA+CeDrAH4DwA8A+C7O3XnHxvtLKfG1NOfnrK4zMhfLvHbJsUqfSpm1/PV6cNqsDgZaJBAnlRSj\nz+vzRspHL+f8lDuvXq/P+P957ga8CxRJ/IVU/RVvXjrip3zwAGaCX/QYDodm0owMaUChq97wPDXE\nkJe6txj2irC1tWVKcpH0YpkvGpr81twK4hGrfarghlwd57gKVd9xSRT54q1ad3KVdFk5q4uBTq6S\nGmtVwbHq2uthBeBcNuS2qLQ1R89ZQ4iv3XE89Nmdr5xuy2R3df9ycOJfM4pIb/Wc43h6aTf16NGj\nbGji62AeVuVT90hpJYuCyc9lrfmMvru7aw5R14tIr8N5eW4V0HRX3eXhxL8mpMiVqkgjxjYJyOl2\nuzg5OcHx8THeeustvPnmm3jjjTfQ6XRmgm44EGeZNtHLkl7Xs5fzuSTK7O/vz4x2uz3jf+fHElpr\n+eAt1yGvy7E4nPgK80gwj1Sa3PrxvLr1nU4nayop4/j4ONdosqh89WX98JbULPLDc4acjpsXP7yQ\n/ODgIEf6g4MDtFqtmTBafjzPHecEvxo48RcAbwYcOVdUPy51LYqDH4/H6PV6OdWeu8oOBoNcY8rL\nWOMZVm95eWy56HiwO04nyLAfntV7jrHn2HytvjtuDk78AljqMAfYWAUndatoa4MoGhJrz0Ni7K3y\n1Zf1xet69FZtestab/nhi6LvOD2W/fCizvNZ3uvi3Tyc+AlYPngAGXlTLrV5deWLGlmMx+OsQo7O\nrGPi6wCcVazyenDCjBUPb7nY2P3GbjsrnVYCcLTxzkl/s3DiF8AKxhGJz1VqeQwGA7P6rH7MZOfn\nrBZVVp85fYS4bAuqVJRdKq+d1XrL957yyy/ih3d33M3DiW/Ayo5j4jNBxe3G+e58BLCumvB81amy\nRWmzVhWceSiyyks+PBeu5JBZ9sNbmkCqUy1frZr3TvqbhxM/gZQLTNel5zTYk5MTdLvdQvIW9ZWz\njgDaEGi565Y17mnVnt1xYpSzjHS8Ueir7lCbUustd52T/+bgxF8QOnuOA23E3358fJyVoE5F11lS\nX0fWWVF1Mpe18FXPi6AlPkfc6YKV+/v7OX+8FLC00mE5H569BVY1Xcv/7qS/WWwc8TUBrMeWJLck\nqFVJttfr5UJptb/dIj5vAPOk/WXO6hbm+eRFndeGN7lK+qsVfLO/v5+lxaZUejbU6TXwc47bxcYR\nHyju2qJ96zqcNWU4k3mv18tIzoQXVV9KW1nqvm41ZZWzuiw0keblw8+rSy/Vb3jw2Z5z31NhtLwu\nJ/p6YiOJD8zmvMtjOaOztNXzVJcYcbexf53np6enWeqrVaGW1XlN/mVgkUvXstOFKri8FV+1MU+P\nVD68lRPvpF9/bCTxUyo7W+S1tZwls+Vv1372VDcadudZfnpLy1imG01KnZbEmVQSjFjtiwjOTSr4\nGMDn+JQ7zuPo7wY2jvj6vM7k0oY5q4qNVsu15JYqN3pItRsdYGNF76Uq1y6K1NlZDGhitBPprknM\n5ar1VafNat98tVo1m1U66e8WNo74QLpTrHbFMWFlLtF3+nxulbLWXWrEeGfF6GuiWwbEy5LfMuAJ\n8UW6swpv1avXo9lsFrawYj+8FevvpL8b2EjiA7OVbkTqMvGtFtESGjtvpBpapJpOFNXZW9awB8xa\n7TlHXpJmrLx47auXq8TSW354XbraffF3FxtJ/FTRC12LnqvccIUbXdOOxzw/PJ/XU9ei+WVgpc1a\nEp9TZXXmnB71ej0pzd0PvznYOOJrwmuLPfeWY4kvrjkx0KWGzsDTkXVM4qIzr7aCL0ogTUhNzGaz\nORN4I3nx3D46JfVrtZp5jNBzx93GxhJfyKjP6Lr8tGWdTxWp1KmwOlZeUKQGW6SdF7Ou3XVFpak4\nCIfbT8lczvmcLSeJM26ZLw82jvjARSIN94qXsFmrN7yQnnvL6ZFyx+kz+jyCF5GWy05ZkM8typdv\nNBozRjyes5VfZ8x5AE55sHHEZ4nPraO1ip+S+L1eb24sfZG0BzBDcqvhRKrxBFeisUjH8fW6dJWE\n41r16GXo0tVa4vN9nfSbi40mvqj63IFGk15vAL1eL1kPj7PjrJTYGGNO4lsSXZej1uS1ilLwY+kh\nn+pEk/qZbiHNQ9bnan55sHHEB87Jr1V9LpoxT+JbMfta0qfccZr4uroNV6S1ilVw/3aLfNVq1Yyq\n42YVVvac9sXrdtKpVlRO+s3ExhG/SOJzwE7qjD8YDMwKudoXL/eyfPB8xuf010U6zcg5nz+Lr7Va\nzYy1lyAd8cOnSmtZxw+Zu4pfHmw08ZeR+IPBwPS/W75467El8fkML5JdV7qRM3itVst9lr7WarXC\nyDvxwxcNXqcH35QTG0d8RpErTReiEEKuCilBnRqp7Di5VqvVpGV9EeKzH97J7Uhh44hvha3q1NcQ\nwkwiy+7uLvb39zEcDle6v9w7NaxCljyq1WpunfparVZnylVzDL0T3bEINpL4OkNN1HEt6XXLp06n\ng9FotNL9OR9eG/b4vikrvBj3UlZ1bdxLBeC4Vd5RhLnEDyE8B+AlAM8AOAPwyzHGz4YQXgDwEQCv\nTV/6yRjj717bShcES/x6vZ6ztvOZm0nP/eevivhWoUntzmMLvMylhVTKui5x+FbJaivyzg12DguL\nSPwxgJ+LMX4zhLAD4I9CCL83/dmLMcYXr295lwcTXAxlrNoLYQaDAXZ2dmb6yy/SI36R+xd1qkkF\n70gAT1HYrtgQdGUdjvpzsjvmYS7xY4yvAnh1Ou+EEF4G8I7pj9fuP4slPnCh3tdqNYxGo0zSp2re\nXwXxiyzqKU1ArkU95PgYs2j5Kz13OAAgXLL4ww8BOALwtwF8DMCHATwG8IcAPhZjfGy8Jy6bdroM\nUsU0dXpuUcPLVWAZ11gFL9oQFmksoV/Ppa901N+8hB/HZiOEgBij+QdfmPhTNf8IwL+JMX45hPA0\ngDdijDGE8PMA/lqM8aeN990o8ecVu0iNZereFaFI8loq+aIWeOs9y7zfsfkoIv5CVv0QQgXAbwH4\n9zHGLwNAjPF1esnnAPxO6v0PHjzI5oeHhzg8PFzktkvBXViOsuLo6AhHR0cLvXYhiR9CeAnn0v3n\n6Llnp+d/hBD+JYAfjjF+0HjvjUp8h8NxjpVU/RDCewD8PoBvAYjT8UkAHwTwPM5dfK8A+JkY40Pj\n/U58h+MWcCVn/BVu7sR3OG4BRcRP+44cDsfGwonvcJQQTnyHo4Rw4jscJYQT3+EoIZz4DkcJ4cR3\nOEoIJ77DUUI48R2OEsKJ73CUEE58h6OEcOI7HCXEjRN/0Xzh24KvbzWs8/rWeW3Aza7Pia/g61sN\n67y+dV4bsOHEdzgctw8nvsNRQtxIIY5rvYHD4Uji1irwOByO9YOr+g5HCeHEdzhKiBsjfgjhvSGE\nb4cQvhNC+PhN3XdRhBBeCSH8rxDC/wwhfH0N1vP5EMLDEML/pufuhRC+GkL40xDCfw4h7K/Z+l4I\nIXwvhPA/puO9t7i+50II/zWE8H9CCN8KIfyL6fNr8Ts01vfPp8/fyO/wRs74IYQtAN8B8GMA/grA\nNwB8IMb47Wu/+YIIIfxfAH83xvjottcCACGEvwegA+ClGOO7ps/9IoA3Y4yfnm6e92KMn1ij9b0A\n4HQdGqmGEJ4F8Cw3ewXwPgA/hTX4HRas7x/hBn6HNyXx3w3gz2KM340xjgD8Gs6/5DohYI2OPjHG\nrwHQm9D7AHxxOv8igH94o4siJNYHrEkj1RjjqzHGb07nHQAvA3gOa/I7TKzvxprR3tQ/+jsA/AU9\n/h4uvuS6IAL4vRDCN0IIH7ntxSTwdmlaMu1i9PZbXo+Fj4YQvhlC+He3eRRhTJu9Pg/gDwA8s26/\nQ1rff58+de2/w7WRcGuA98QY/w6AfwDgn01V2XXHuvlifwnA34gxPo/z1urroPLv4Lzv489OJav+\nnd3q79BY3438Dm+K+H8J4Afp8XPT59YGMcbvT6+vA/htnB9P1g0PQwjPANkZ8bVbXk8OMcbXqW3S\n5wD88G2ux2r2ijX6Haaa0d7E7/CmiP8NAH8zhPDOEEINwAcAfOWG7j0XIYTWdOdFCKEN4McB/PHt\nrgrA+VmPz3tfAfDh6fxDAL6s33DDyK1vSiTBT+L2f4e/AuBPYoyfoefW6Xc4s76b+h3eWOTe1C3x\nGZxvNp+PMf7Cjdx4AYQQ/jrOpXzEeevwX73t9YUQvgTgEMB9AA8BvADgPwD4TQA/AOC7AN4fYzxe\no/X9KBZopHpD60s1e/06gN/ALf8OV21Gu/L9PWTX4Sgf3LjncJQQTnyHo4Rw4jscJYQT3+EoIZz4\nDkcJ4cR3OEoIJ77DUUI48R2OEuL/AytVNTNhmicLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd002f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recover_img = np.reshape(first[0], (28, 28)) #這一段把資料還原成 28 * 28\n",
    "plt.imshow(recover_img, cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以我們現在大概知道圖片的樣子，接下來就是...傳說中neural network的實現，現在就只有一層的hidden layer(傳說中黑盒子的地方)，而所謂的deep learning，則是2層或以上的neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 這個是類神經網路的學習函數，其實不一定要用這個，但是它具有一個很特別的性質，至於是什麼就請google吧，解釋起來很長一串 XD\n",
    "def sigmoid(z):\n",
    "    \"\"\" The sigmoid function. \"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\" Derivative of the sigmoid function. \"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是實現的過程，建議先不要深入下去看，大概了解就好，有興趣之後去http://neuralnetworksanddeeplearning.com/chap1.html\n",
    "<br>是原封不動搬過來的，裡面從頭到尾說清楚講明白</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        starttime = datetime.datetime.now()\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print \"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test)\n",
    "            else:\n",
    "                print \"Epoch {0} complete\".format(j)\n",
    "        print(\"\\nDone!\\nModel trained in %s\" % (datetime.datetime.now() - starttime))  \n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = Network([784, 20, 10])\n",
    "# 因為是28 * 28的圖片，可以把每一個pixel想象成一個input，所以有784個input,\n",
    "# 而 10是因為有10個號碼所以最後要告訴我到底是哪一個，20就是傳說中的黑盒子函數，隨便用20個類神經元去模擬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 8439 / 10000\n",
      "Epoch 1: 8800 / 10000\n",
      "Epoch 2: 8944 / 10000\n",
      "Epoch 3: 9007 / 10000\n",
      "Epoch 4: 9082 / 10000\n",
      "Epoch 5: 9117 / 10000\n",
      "Epoch 6: 9145 / 10000\n",
      "Epoch 7: 9171 / 10000\n",
      "Epoch 8: 9185 / 10000\n",
      "Epoch 9: 9181 / 10000\n",
      "\n",
      "Done!\n",
      "Model trained in 0:01:50.053000\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 10, 20, 1.5 , test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面那行其實包含很多，模型訓練、然後再把一開始的測試資料test_data餵給訓練好的model，最後再evaluate這個模型，讓演算法跑10次就（餵給SGD的第二個參數）停下來，結果就已經發現有91.8%的準確度了!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 其實還有很多事情可以做，下次再給你看我真正有做的東西吧!因為都沒好好整理，所以沒有一個夠完整的project可以拿出來說嘴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
